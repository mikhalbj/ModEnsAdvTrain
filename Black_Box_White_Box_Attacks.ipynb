{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZtk4boC4VaJ"
      },
      "source": [
        "# Set-up\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBzUXfLEwn_R"
      },
      "source": [
        "Sources for pre-trained models on CIFAR-10\n",
        "\n",
        "https://github.com/chenyaofo/pytorch-cifar-models/tree/master/pytorch_cifar_models\n",
        "\n",
        "https://github.com/huckiyang/PyTorch-CIFAR10\n",
        "\n",
        "https://github.com/huyvnphan/PyTorch_CIFAR10\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71f8dAoKqmwr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import time\n",
        "import pickle\n",
        "from torchvision import datasets, transforms, utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.optimize\n",
        "import multiprocessing\n",
        "from six.moves import urllib\n",
        "import multiprocessing as mp\n",
        "from IPython.core import profiledir\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd2Mc3wKjBFp",
        "outputId": "eb54dbd4-f508-450f-a36a-59df79c73752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Final Project')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21HgvE_Dm6a4"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Final Project/models')\n",
        "\n",
        "from vgg import vgg11_bn, vgg13_bn, vgg16_bn, vgg19_bn\n",
        "from resnet import resnet18, resnet34, resnet50\n",
        "from densenet import densenet121, densenet161, densenet169\n",
        "from mobilenetv2 import mobilenet_v2\n",
        "from googlenet import googlenet\n",
        "from inception import inception_v3\n",
        "\n",
        "#os.chdir('/content/drive/MyDrive/Final Project')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm2uyS_Zi5Jv",
        "outputId": "67b200c7-92f8-4f2c-921b-7025513db6e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "batch_size_cifar = 100\n",
        "\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2471, 0.2435, 0.2616)\n",
        "\n",
        "t = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "def cifar_loaders(batch_size, shuffle_test=False): \n",
        "    data_dir = './data'\n",
        "    train = datasets.CIFAR10(data_dir, train=True, download=True, \n",
        "        transform=t)\n",
        "    \n",
        "    test = datasets.CIFAR10(data_dir, train=False, \n",
        "        transform=t)\n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
        "        shuffle=True, pin_memory=True)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size,\n",
        "        shuffle=shuffle_test, pin_memory=True)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "train_cifar_loader, test_cifar_loader = cifar_loaders(batch_size_cifar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHJ9l39HlxOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5924292f-6566-40c1-8dda-f59925377bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.)\n",
            "tensor(0.)\n"
          ]
        }
      ],
      "source": [
        "data, label = next(iter(train_cifar_loader))\n",
        "print(torch.max(data))\n",
        "print(torch.min(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D0w3djTi9h_"
      },
      "outputs": [],
      "source": [
        "# Using weights from https://github.com/huyvnphan/PyTorch_CIFAR10\n",
        "vgg16_bn_mod = vgg16_bn(pretrained = True).to(device).eval()\n",
        "vgg11 = vgg11_bn(pretrained = True).to(device).eval()\n",
        "resnet18_mod = resnet18(pretrained = True).to(device).eval()\n",
        "resnet34_mod = resnet34(pretrained = True).to(device).eval()\n",
        "resnet50_mod = resnet50(pretrained = True).to(device).eval()\n",
        "densenet121_mod = densenet121(pretrained = True).to(device).eval()\n",
        "densenet161_mod = densenet161(pretrained = True).to(device).eval()\n",
        "densenet169_mod = densenet169(pretrained = True).to(device).eval()\n",
        "mobilenet_v2_mod = mobilenet_v2(pretrained = True).to(device).eval()\n",
        "googlenet_mod = googlenet(pretrained = True).to(device).eval()\n",
        "inception_v3_mod = inception_v3(pretrained = True).to(device).eval()\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/Final Project')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSUXnJhrN3Xr"
      },
      "outputs": [],
      "source": [
        "all_models = [#vgg11, vgg13, vgg16, vgg19,\n",
        "          resnet18_mod, resnet34_mod, resnet50_mod,\n",
        "          densenet121_mod, densenet161_mod, densenet169_mod,\n",
        "          mobilenet_v2_mod, googlenet_mod, inception_v3_mod]\n",
        "\n",
        "#for model in all_models:\n",
        "#  testClean(test_cifar_loader, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CuLlnpWvZNN"
      },
      "outputs": [],
      "source": [
        "t2 = transforms.Compose([\n",
        "    # transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "resnet50_mod2 = lambda x: resnet50_mod(t2(x))\n",
        "densenet169_mod2 = lambda x: densenet169_mod(t2(x))\n",
        "mobilenet_v2_mod2 = lambda x: mobilenet_v2_mod(t2(x))\n",
        "inception_v3_mod2 = lambda x: inception_v3_mod(t2(x))\n",
        "resnet18_mod2 = lambda x: resnet18_mod(t2(x))\n",
        "googlenet_mod2 = lambda x: googlenet_mod(t2(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj8NnNQw4cmv"
      },
      "source": [
        "# Implementing Min-Max for Adversarial Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vURFdWRsceOH",
        "outputId": "1337a97a-b708-455c-f13a-93c748601b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "640.23876953125\n",
            "648.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:393: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.memory_allocated() / 1024**2)\n",
        "print(torch.cuda.memory_cached() / 1024**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKqCvH259XH8"
      },
      "source": [
        "# Trying with the new transform order\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9lRVmLS4MHO"
      },
      "source": [
        "# Multiprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dxzXXYH7c1t",
        "outputId": "56109c0c-5b74-4c5f-9a69-467fa8905817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished in 12.76 seconds\n"
          ]
        }
      ],
      "source": [
        "# reference: https://stackoverflow.com/questions/42490368/appending-to-the-same-list-from-different-processes-using-multiprocessing\n",
        "\n",
        "from multiprocessing import Process, Manager\n",
        "\n",
        "# or instead use torch.multiprocessing\n",
        "\n",
        "from torch.multiprocessing import Pool, Process, set_start_method\n",
        "try:\n",
        "      set_start_method('spawn')\n",
        "except RuntimeError:\n",
        "      pass\n",
        "\n",
        "start = time.perf_counter()\n",
        "\n",
        "\n",
        "def create_examples(L1, L2, i, data_point):\n",
        "    data, label = data_point\n",
        "    data_mod = adversarial_perturbation_in_ex(data,label,models,criterion, .005, 5, .0625, 20,y_n,.4, gamma = 0.03)\n",
        "    L1.append(data_mod.data)\n",
        "    L2.append(label)\n",
        "\n",
        "L1 = []\n",
        "L2 = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with Manager() as manager:\n",
        "        L1 = manager.list()\n",
        "        L2 = manager.list()\n",
        "        processes = []\n",
        "\n",
        "        for i, data_point in enumerate(train_cifar_loader):\n",
        "            p = multiprocessing.Process(target=create_examples, args = (L1, L2, i, data_point))\n",
        "            #if i == 50:\n",
        "            #  break\n",
        "            p.start()\n",
        "            if i == 50:   # trying to see if there's a difference in where I put this break\n",
        "              break\n",
        "            processes.append(p)\n",
        "\n",
        "        for process in processes:\n",
        "            process.join()\n",
        "\n",
        "        L1 = list(L1)\n",
        "        L2 = list(L2)\n",
        "\n",
        "finish = time.perf_counter()\n",
        "\n",
        "\n",
        "print(f'Finished in {round(finish-start, 2)} seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPVc0hbM5bS4",
        "outputId": "aaa0827c-1713-4b94-d92b-76374f2d2989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.5347, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.1134, 0.1134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.5643, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.2422, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.1295, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[1.0000, 0.9166, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.5118, 0.4069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.2120, 0.2570, 0.0000,  ..., 0.6917, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]]), tensor([[[[0.3760, 0.4078, 0.4554,  ..., 0.6141, 0.5823, 0.5347],\n",
            "          [0.4078, 0.4395, 0.5030,  ..., 0.6617, 0.6141, 0.5823],\n",
            "          [0.3919, 0.4395, 0.4871,  ..., 0.6299, 0.5982, 0.5665],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.4033, 0.4355, 0.4838,  ..., 0.6609, 0.6287, 0.5804],\n",
            "          [0.4355, 0.4677, 0.5321,  ..., 0.6931, 0.6609, 0.6287],\n",
            "          [0.4194, 0.4677, 0.5160,  ..., 0.6770, 0.6448, 0.6126],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.5868, 0.6168, 0.6617,  ..., 0.7966, 0.7517, 0.7067],\n",
            "          [0.6168, 0.6467, 0.7067,  ..., 0.8266, 0.7817, 0.7517],\n",
            "          [0.6018, 0.6317, 0.6917,  ..., 0.7966, 0.7667, 0.7367],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]]), tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0269, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0745, 0.0000, 0.0427,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.2744, 0.1778, 0.2583,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.3549, 0.2583, 0.2422,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.4033, 0.2744, 0.3388,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0489, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0006, 0.0812, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0328, 0.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]]), tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [1.0000, 1.0000, 0.8045,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [1.0000, 1.0000, 0.7898,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [1.0000, 1.0000, 0.7217,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]]), tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [1.0000, 1.0000, 0.8045,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [1.0000, 1.0000, 0.7898,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [1.0000, 1.0000, 0.7217,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]]), tensor([[[[0.8363, 0.8521, 0.8680,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.8839, 0.8521, 0.8997,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.8045, 0.8363, 0.8839,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.7092, 0.7415, 0.7254,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.7415, 0.7415, 0.7737,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.6609, 0.6931, 0.7415,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]]), tensor([[[[0.8363, 0.8521, 0.8680,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.8839, 0.8521, 0.8997,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.8045, 0.8363, 0.8839,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.7092, 0.7415, 0.7254,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.7415, 0.7415, 0.7737,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.6609, 0.6931, 0.7415,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          ...,\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])]\n",
            "[tensor([1]), tensor([0]), tensor([5]), tensor([5]), tensor([5]), tensor([5]), tensor([5])]\n"
          ]
        }
      ],
      "source": [
        "print(L1)\n",
        "print(L2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "K7XytlxP8SDj",
        "outputId": "4ba7f682-8050-4bcf-adcf-932bbe27f588"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7fed25dd90>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 144x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# trying to view the above saved images\n",
        "\n",
        "with open(f\"perturbed_imgs_J.pkl\", 'wb') as f:\n",
        "  pickle.dump(L1, f)\n",
        "\n",
        "file = open('perturbed_imgs_J.pkl', 'rb')\n",
        "images_J = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "\n",
        "with open(f\"labels_trial_J.pkl\", 'wb') as f:\n",
        "  pickle.dump(L2, f)\n",
        "\n",
        "file = open('labels_trial_J.pkl', 'rb')\n",
        "labels_J = pickle.load(file)\n",
        "\n",
        "\n",
        "grid = torchvision.utils.make_grid(images_J[6]).cpu()\n",
        "plt.figure(figsize = (2,4))\n",
        "plt.imshow(np.transpose(torchvision.utils.make_grid(grid.to(device)[:128], padding=2, normalize=True).cpu(),(1,2,0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQASXHVo4vdS"
      },
      "source": [
        "# Image Saving and Dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtCrW5s_th01"
      },
      "outputs": [],
      "source": [
        "# to store and read back in the adversarial examples and labels\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open(f\"perturbed_imgs_trial2.pkl\", 'wb') as f:\n",
        "  pickle.dump(perturbed_list, f)\n",
        "\n",
        "file = open('perturbed_imgs_trial2.pkl', 'rb')\n",
        "images = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "\n",
        "with open(f\"labels_trial2.pkl\", 'wb') as f:\n",
        "  pickle.dump(label_list, f)\n",
        "\n",
        "file = open('labels_trial2.pkl', 'rb')\n",
        "labels = pickle.load(file)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3UUqehjmpqC"
      },
      "outputs": [],
      "source": [
        "grid = torchvision.utils.make_grid(images[0]).cpu()\n",
        "plt.figure(figsize = (2,4))\n",
        "plt.imshow(np.transpose(torchvision.utils.make_grid(grid.to(device)[:128], padding=2, normalize=True).cpu(),(1,2,0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzX5f0oLETtB"
      },
      "outputs": [],
      "source": [
        "# To create the dataset\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, labels, images, transform=None, target_transform=None):\n",
        "        self.labels = labels\n",
        "        self.images = images\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        image = self.images[idx]\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M11nRRWdEcL4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "c5890b33-f5b1-4e5c-89a1-3e2a6e2ea16e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e0f967a0c1b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrial_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrial_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0;32m--> 108\u001b[0;31m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ],
      "source": [
        "# to use the dataset as dataloader\n",
        "\n",
        "trial_ds = CustomImageDataset(labels, images)\n",
        "trial_loader = torch.utils.data.DataLoader(trial_ds, batch_size = 1, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7jLO8rUEr-c"
      },
      "outputs": [],
      "source": [
        "# Checking that the dataloader works \n",
        "\n",
        "def testClean2(test_loader, model):\n",
        "  model.eval()\n",
        "  correct_count = 0\n",
        "  total_count = 0\n",
        "  for data in test_loader:\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    #print(\"image size:\", images.size())\n",
        "    images = torch.squeeze(images) # This code is identical to testClean except for the squeeze (an extra dimension was added during the list process)\n",
        "    #print(\"image after squeeze:\", images.size())\n",
        "    #print(\"labels: \", labels.size())\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total_count += labels.size(0)\n",
        "    correct_count += (predicted == labels).sum().item()\n",
        "\n",
        "  acc = float(correct_count/total_count)\n",
        "\n",
        "  print(\"Accuracy on test set is {}\".format(acc))\n",
        "  return acc\n",
        "\n",
        "# testClean2(trial_loader, inception_v3_mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Training\n"
      ],
      "metadata": {
        "id": "ht1Xid2meE3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Training VGG on the adversarial images\n",
        "\n",
        "#Import one of the adversarial example sets\n",
        "os.chdir('/content/drive/MyDrive/Final Project/Adversarial Examples')\n",
        "\n",
        "import pickle\n",
        "\n",
        "file = open('perturbed_imgs_combo_0.pkl', 'rb')\n",
        "perturbed_imgs_combo_0 = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "\n",
        "file = open('labels_combo_0.pkl', 'rb')\n",
        "labels_combo_0 = pickle.load(file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "LiEwUKPOBHjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(6):\n",
        "  file = open(f\"perturbed_imgs_combo_{i}.pkl\", 'rb')\n",
        "  perturbed_imgs_combo_0 = pickle.load(file)\n",
        "  file.close()\n",
        "\n",
        "\n",
        "  file = open(f\"labels_combo_{i}.pkl\", 'rb')\n",
        "  labels_combo_0 = pickle.load(file)\n",
        "  file.close()"
      ],
      "metadata": {
        "id": "LCuvUh0l1Tnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the dataset\n",
        "\n",
        "ds_combo_0 = CustomImageDataset(labels_combo_0, perturbed_imgs_combo_0)\n",
        "train_size = int(0.8 * len(ds_combo_0))\n",
        "test_size = len(ds_combo_0) - train_size\n",
        "train_dataset_0, test_dataset_0 = torch.utils.data.random_split(ds_combo_0, [train_size, test_size])\n",
        "\n",
        "train_loader_combo_0 = torch.utils.data.DataLoader(train_dataset_0, batch_size = 1, shuffle = True)\n",
        "test_loader_combo_0 = torch.utils.data.DataLoader(test_dataset_0, batch_size = 1, shuffle = True)"
      ],
      "metadata": {
        "id": "j-M9oHkpCHEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing performance of pretrained VGG on adversarial images from combo 0\n",
        "\n",
        "testClean2(test_loader_combo_0, vgg16_bn_mod)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aypc-xgFHY4",
        "outputId": "fe2b6059-12cf-44d0-d8de-85a64b6f1a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set is 48.97\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48.97"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy of traditional AT\n",
        "os.chdir('/content/drive/MyDrive/Final Project/models')\n",
        "\n",
        "vgg11_adv = vgg11_bn()\n",
        "vgg11_adv.load_state_dict(torch.load('vgg11_adv.pt'))\n",
        "vgg11_adv.to(device)\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/Final Project/models/state_dicts')\n",
        "vgg11_ens_adv_indv = vgg11_bn()\n",
        "vgg11_ens_adv_indv.load_state_dict(torch.load('vgg11_adv_ens_3indiv.pt'))\n",
        "vgg11_ens_adv_indv.to(device)\n",
        "# clean images\n",
        "testClean2(test_cifar_loader, vgg11_adv)\n",
        "\n",
        "# combo 0\n",
        "testClean2(test_loader_combo_0, vgg11_adv)\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/Final Project')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "7w_tu2CpDpWw",
        "outputId": "9a9b2c24-13bd-4ed7-e73d-8e84ee9158c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a17931ad87ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mvgg11_ens_adv_indv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# clean images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtestClean2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_cifar_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvgg11_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# combo 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'testClean2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg11_adv = vgg11_bn()\n",
        "#vgg11_adv.load_state_dict(torch.load('vgg11_adv.pt'))\n",
        "vgg11_adv.to(device)\n",
        "\n",
        "testClean2(test_loader_combo_0, vgg16_bn_mod)"
      ],
      "metadata": {
        "id": "MiG1eNDfDwra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd83875f-606d-4e29-e197-c31a8a67913e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set is 48.8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48.8"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Take two with different batch sizes\n",
        "\n",
        "#Create the dataset\n",
        "\n",
        "ds_combo_0 = CustomImageDataset(labels_combo_0, perturbed_imgs_combo_0)\n",
        "\n",
        "train_size = int(0.8 * len(ds_combo_0))\n",
        "test_size = len(ds_combo_0) - train_size\n",
        "train_dataset_0, test_dataset_0 = torch.utils.data.random_split(ds_combo_0, [train_size, test_size])\n",
        "\n",
        "train_loader_combo_0 = torch.utils.data.DataLoader(train_dataset_0, batch_size = 1, shuffle = True)\n",
        "test_loader_combo_0 = torch.utils.data.DataLoader(test_dataset_0, batch_size = 1, shuffle = True)"
      ],
      "metadata": {
        "id": "bqHKqpYQPL6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing performance of adversarially trained VGG on adversarial images from combo 0\n",
        "\n",
        "testClean2(test_loader_combo_0, model_adv2)"
      ],
      "metadata": {
        "id": "JwHt5GvgPVKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjKSYI527TIj"
      },
      "source": [
        "# Black Box Attacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZl5ohL7uodZ"
      },
      "outputs": [],
      "source": [
        "# WHITE BOX ATTTACK STEP-LL METHOD \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def optimize_linear(grad, eps, ord=np.inf):\n",
        "  red_ind = list(range(1, len(grad.size())))\n",
        "  avoid_zero_div = torch.tensor(1e-12, dtype=grad.dtype, device=grad.device)\n",
        "  if ord == np.inf:\n",
        "    optimal_perturbation = torch.sign(grad)\n",
        "  elif ord == 1:\n",
        "    abs_grad = torch.abs(grad)\n",
        "    sign = torch.sign(grad)\n",
        "    red_ind = list(range(1, len(grad.size())))\n",
        "    abs_grad = torch.abs(grad)\n",
        "    ori_shape = [1]*len(grad.size())\n",
        "    ori_shape[0] = grad.size(0)\n",
        "\n",
        "    max_abs_grad, _ = torch.max(abs_grad.view(grad.size(0), -1), 1)\n",
        "    max_mask = abs_grad.eq(max_abs_grad.view(ori_shape)).to(torch.float)\n",
        "    num_ties = max_mask\n",
        "    for red_scalar in red_ind:\n",
        "      num_ties = torch.sum(num_ties, red_scalar, keepdim=True)\n",
        "    optimal_perturbation = sign * max_mask / num_ties\n",
        "    opt_pert_norm = optimal_perturbation.abs().sum(dim=red_ind)\n",
        "    assert torch.all(opt_pert_norm == torch.ones_like(opt_pert_norm))\n",
        "  elif ord == 2:\n",
        "    square = torch.max(\n",
        "        avoid_zero_div,\n",
        "        torch.sum(grad ** 2, red_ind, keepdim=True)\n",
        "        )\n",
        "    optimal_perturbation = grad / torch.sqrt(square)\n",
        "    opt_pert_norm = optimal_perturbation.pow(2).sum(dim=red_ind, keepdim=True).sqrt()\n",
        "    one_mask = (square <= avoid_zero_div).to(torch.float) * opt_pert_norm + \\\n",
        "            (square > avoid_zero_div).to(torch.float)\n",
        "    assert torch.allclose(opt_pert_norm, one_mask, rtol=1e-05, atol=1e-08)\n",
        "\n",
        "  scaled_perturbation = eps * optimal_perturbation\n",
        "  return scaled_perturbation\n",
        "\n",
        "def least_likely_class_method(model_fn, x, eps, ord = np.inf,\n",
        "                         clip_min=None, clip_max=None, sanity_checks=False):\n",
        "    asserts = []\n",
        "    if clip_min is not None:\n",
        "      assert_ge = torch.all(torch.ge(x, torch.tensor(clip_min, device=x.device, dtype=x.dtype)))\n",
        "      asserts.append(assert_ge)\n",
        "    if clip_max is not None:\n",
        "      assert_le = torch.all(torch.le(x, torch.tensor(clip_max, device=x.device, dtype=x.dtype)))\n",
        "      asserts.append(assert_le)\n",
        "    x = x.clone().detach().to(torch.float).requires_grad_(True)\n",
        "     \n",
        "    _, y_ll = torch.min(model_fn(x), 1)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    loss = loss_fn(model_fn(x), y_ll)\n",
        "    loss = -loss\n",
        "    model_fn.zero_grad()\n",
        "    loss.backward()\n",
        "    optimal_perturbation = optimize_linear(x.grad, eps, ord)\n",
        "    adv_x = x + optimal_perturbation\n",
        "    adv_x = torch.clamp(adv_x, clip_min, clip_max)\n",
        "    return adv_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "id": "O1Im8ioJMLmn",
        "outputId": "170f3504-f6a3-4faa-d609-51afa0397256"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.9.tar.gz (16 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n",
            "Collecting ipython>=7.17.0\n",
            "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 41.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n",
            "Collecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.0.10)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (1.15.0)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11649 sha256=102da1efad75d8ef384f944a1ee462df3b32b0255aa4969522a1d101cdcbbd11\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/cd/cc/aaf92acae337a28fdd2aa4d632196a59745c8c39f76eaeed01\n",
            "Successfully built ipdb\n",
            "Installing collected packages: matplotlib-inline, jedi, ipython, ipdb\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.9.0\n",
            "    Uninstalling ipython-7.9.0:\n",
            "      Successfully uninstalled ipython-7.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython~=7.9.0, but you have ipython 7.34.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipdb-0.13.9 ipython-7.34.0 jedi-0.18.1 matplotlib-inline-0.1.6\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "ogGM9L1ouuBW",
        "outputId": "51e52673-29ac-41d1-f5ea-a64fd1913d15"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-aa6800145873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# os.mkdir(os.path.join(\"image\", folder))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mboundary_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-aa6800145873>\u001b[0m in \u001b[0;36mboundary_attack\u001b[0;34m(classifier, initial_sample, target_sample, attack_class, target_class)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtrial_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_sample\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mforward_perturbation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversarial_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mn_calls\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mattack_class\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/.shortcut-targets-by-id/1rYTOKvU8QrKxfh_WvNF4Bea_mj4rP1uD/Final Project/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
          ]
        }
      ],
      "source": [
        "# BOUNDARY BLACK BOX ATTACK\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms, datasets\n",
        "#from resnet import *\n",
        "\n",
        "def orthogonal_perturbation(delta, prev_sample, target_sample):\n",
        "\tperturb = np.random.randn(1, 3, 32, 32)\n",
        "\tperturb /= np.linalg.norm(perturb, axis=(2, 3))\n",
        "\tperturb *= delta * np.mean(get_diff(target_sample, prev_sample))\n",
        "\n",
        "\tdiff = (target_sample - prev_sample).astype(np.float32) \n",
        "\tdiff /= get_diff(target_sample, prev_sample)\n",
        "\tperturb -= (np.vdot(perturb, diff) / np.linalg.norm(diff)**2) * diff\n",
        "\n",
        "\toverflow = (prev_sample + perturb) - 1\n",
        "\tperturb -= overflow * (overflow > 0)\n",
        "\tunderflow = -(prev_sample + perturb)\n",
        "\tperturb += underflow * (underflow > 0)\n",
        "\treturn perturb\n",
        "\n",
        "\n",
        "def forward_perturbation(epsilon, prev_sample, target_sample):\n",
        "\tperturb = target_sample - prev_sample\n",
        "\tperturb *= epsilon\n",
        "\treturn perturb\n",
        "\n",
        "\n",
        "def get_diff(sample_1, sample_2):\n",
        "\treturn np.linalg.norm(sample_1 - sample_2, axis=(2, 3))\n",
        "\n",
        "\n",
        "def boundary_attack(classifier, initial_sample, target_sample, attack_class, target_class):\n",
        "    adversarial_sample = initial_sample\n",
        "    n_steps = 0\n",
        "    n_calls = 0\n",
        "    epsilon = 1.\n",
        "    delta = 0.1\n",
        "\n",
        "    while True:\n",
        "        trial_sample = adversarial_sample + forward_perturbation(epsilon, adversarial_sample, target_sample)\n",
        "        prediction = classifier(trial_sample)\n",
        "        n_calls += 1\n",
        "        if torch.argmax(prediction) == attack_class:\n",
        "            adversarial_sample = trial_sample\n",
        "            break\n",
        "        else:\n",
        "            epsilon *= 0.9\n",
        "\n",
        "    # Iteratively run attack\n",
        "    while True:\n",
        "        print(\"Step #{}...\".format(n_steps))\n",
        "        print(\"\\tDelta step...\")\n",
        "        d_step = 0\n",
        "        while True:\n",
        "            d_step += 1\n",
        "            print(\"\\t#{}\".format(d_step))\n",
        "            trial_samples = []\n",
        "            for i in np.arange(10):\n",
        "                trial_sample = adversarial_sample + orthogonal_perturbation(delta, adversarial_sample, target_sample)\n",
        "                trial_samples.append(trial_sample)\n",
        "\n",
        "            predictions = classifier(trial_samples)\n",
        "            n_calls += 10\n",
        "            predictions = np.argmax(predictions, axis=1)\n",
        "            d_score = np.mean(predictions == attack_class)\n",
        "            if d_score > 0.0:\n",
        "                if d_score < 0.3:\n",
        "                    delta *= 0.9\n",
        "                elif d_score > 0.7:\n",
        "                    delta /= 0.9\n",
        "                adversarial_sample = np.array(trial_samples)[np.where(predictions == attack_class)[0][0]]\n",
        "                break\n",
        "            else:\n",
        "                delta *= 0.9\n",
        "        # Forward step\n",
        "        print(\"\\tEpsilon step...\")\n",
        "        e_step = 0\n",
        "        while True:\n",
        "            e_step += 1\n",
        "            print(\"\\t#{}\".format(e_step))\n",
        "            trial_sample = adversarial_sample + forward_perturbation(epsilon, adversarial_sample, target_sample)\n",
        "\n",
        "            prediction = classifier(trial_sample)\n",
        "            n_calls += 1\n",
        "            if np.argmax(prediction) == attack_class:\n",
        "                adversarial_sample = trial_sample\n",
        "                epsilon /= 0.5\n",
        "                break\n",
        "            elif e_step > 500:\n",
        "                    break\n",
        "            else:\n",
        "                epsilon *= 0.5\n",
        "\n",
        "        n_steps += 1\n",
        "        chkpts = [1, 5, 10, 50, 100, 500]\n",
        "        if (n_steps in chkpts) or (n_steps % 500 == 0):\n",
        "            print(\"{} steps\".format(n_steps))\n",
        "        diff = np.mean(get_diff(adversarial_sample, target_sample))\n",
        "        if diff <= 1e-3 or e_step > 500:\n",
        "            print(\"{} steps\".format(n_steps))\n",
        "            print(\"Mean Squared Error: {}\".format(diff))\n",
        "            break\n",
        "\n",
        "        print(\"Mean Squared Error: {}\".format(diff))\n",
        "        print(\"Calls: {}\".format(n_calls))\n",
        "        print(\"Attack Class: {}\".format(attack_class))\n",
        "        print(\"Target Class: {}\".format(target_class))\n",
        "        print(\"Adversarial Class: {}\".format(np.argmax(prediction)))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    classifier = resnet18_mod\n",
        "\n",
        "    initial_sample, attack_class = next(iter(test_cifar_loader))\n",
        "\n",
        "    target_sample, target_class = next(iter(test_cifar_loader))\n",
        "    # while (attack_class == target_class):\n",
        "    #     target_sample, target_class = next(iter(test_cifar_loader))\n",
        "\n",
        "    # folder = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
        "    # os.mkdir(os.path.join(\"image\", folder))\n",
        "\n",
        "    boundary_attack(classifier, initial_sample, target_sample, attack_class, target_class)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TRANSFER ATTACKO ON RESNET18\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "model = resnet18_mod\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,epsilon = 0.0625, min_val = -1, max_val = 1):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = model(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if images.grad is not None:\n",
        "      images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,min_val,max_val)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = model(Variable(images_adv)) \n",
        "    _,predicted = torch.max(outputs.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)    \n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "#  ITERATIVE - FGSM\n",
        "def i_FGSM(test_loader,iterations = 1,epsilon = 0.1,min_val = -1,max_val = 1):\n",
        "  correct = 0              \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      output_clean = model(Variable(images))\n",
        "    images_adv = Variable(images.data,requires_grad = True)\n",
        "    for j in range(iterations):  \n",
        "      if torch.cuda.is_available():\n",
        "        images_adv = images_adv.cuda()\n",
        "      outputs = model(images_adv)\n",
        "      # outputs2 = model2(images_adv)\n",
        "      # outputs3 = model3(images_adv)\n",
        "      loss = criterion(outputs,Variable(labels))\n",
        "      # loss2 = criterion(outputs2,Variable(labels))\n",
        "      # loss3 = criterion(outputs3,Variable(labels))\n",
        "      model.zero_grad()\n",
        "      # model2.zero_grad()\n",
        "      # model3.zero_grad()\n",
        "      if images_adv.grad is not None:\n",
        "        images.adv.grad.data.fill_(0)\n",
        "      loss.backward()\n",
        "      grad = torch.sign(images_adv.grad.data)  \n",
        "      images_adv = images_adv + (epsilon/iterations)*grad  \n",
        "\n",
        "      images_adv = torch.where(images_adv > images + epsilon,images+epsilon,images_adv)\n",
        "      images_adv = torch.where(images_adv < images-epsilon,images-epsilon,images_adv)\n",
        "      images_adv = torch.clamp(images_adv,min_val,max_val)\n",
        "      images_adv = Variable(images_adv.data,requires_grad = True)\n",
        "      \n",
        "    adv_output = model(Variable(images_adv))\n",
        "    \n",
        "    _,predicted = torch.max(output_clean.data,1)    \n",
        "    _,adv_predicted = torch.max(adv_output.data,1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.015625, min_val = -1,max_val = 1) \n",
        "FGSM(test_cifar_loader, epsilon = 0.015625, min_val = -1, max_val = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvJCN7iM8cvy",
        "outputId": "52871d82-6d0e-4573-a381-2956934b7e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model w/0 adverserial attack on test images is : 38.72 %\n",
            "Accuracy of the model with adverserial attack on test images is : 0.26 %\n",
            "Number of misclassified examples(as compared to clean predictions): 4007/10000\n",
            "Accuracy of the model w/0 adverserial attack on test images is : 38.72 %\n",
            "Accuracy of the model with adverserial attack on test images is : 5.33 %\n",
            "Number of misclassified examples(as compared to clean predictions): 4008/10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFER ATACK ON MOBILENETV2MOD\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "model = mobilenet_v2_mod\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,epsilon = 0.0625, min_val = -1, max_val = 1):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = model(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if images.grad is not None:\n",
        "      images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,min_val,max_val)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = model(Variable(images_adv))  \n",
        "    _,predicted = torch.max(outputs.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)    \n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "#  ITERATIVE - FGSM\n",
        "\n",
        "def i_FGSM(test_loader,iterations = 1,epsilon = 0.1,min_val = -1,max_val = 1):\n",
        "  correct = 0              \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      output_clean = model(Variable(images))\n",
        "    images_adv = Variable(images.data,requires_grad = True)\n",
        "    for j in range(iterations):  \n",
        "      if torch.cuda.is_available():\n",
        "        images_adv = images_adv.cuda()\n",
        "      outputs = model(images_adv)\n",
        "      loss = criterion(outputs,Variable(labels))\n",
        "      model.zero_grad()\n",
        "      if images_adv.grad is not None:\n",
        "        images.adv.grad.data.fill_(0)\n",
        "      loss.backward()\n",
        "      grad = torch.sign(images_adv.grad.data)  \n",
        "      images_adv = images_adv + (epsilon/iterations)*grad  \n",
        "\n",
        "      images_adv = torch.where(images_adv > images + epsilon,images+epsilon,images_adv)\n",
        "      images_adv = torch.where(images_adv < images-epsilon,images-epsilon,images_adv)\n",
        "      images_adv = torch.clamp(images_adv,min_val,max_val)\n",
        "      images_adv = Variable(images_adv.data,requires_grad = True)\n",
        "      \n",
        "    adv_output = model(Variable(images_adv))\n",
        "    _,predicted = torch.max(output_clean.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.0625, min_val = -1,max_val = 1) \n",
        "FGSM(test_cifar_loader, epsilon = 0.0625, min_val = -1, max_val = 1)"
      ],
      "metadata": {
        "id": "8i55fnoNbwQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb7137c0-8296-426f-9e19-5abb65ea18de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model w/0 adverserial attack on test images is : 46.37 %\n",
            "Accuracy of the model with adverserial attack on test images is : 0.0 %\n",
            "Number of misclassified examples(as compared to clean predictions): 4938/10000\n",
            "Accuracy of the model w/0 adverserial attack on test images is : 46.37 %\n",
            "Accuracy of the model with adverserial attack on test images is : 9.25 %\n",
            "Number of misclassified examples(as compared to clean predictions): 5217/10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFER ATTACK ON GOOGLENETMOD\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "model = googlenet_mod\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,epsilon = 0.0625, min_val = -1, max_val = 1):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = model(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if images.grad is not None:\n",
        "      images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,min_val,max_val)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = model(Variable(images_adv)) \n",
        "    _,predicted = torch.max(outputs.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)    \n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  ITERATIVE - FGSM\n",
        "\n",
        "def i_FGSM(test_loader,iterations = 1,epsilon = 0.1,min_val = -1,max_val = 1):\n",
        "  correct = 0              \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      output_clean = model(Variable(images))\n",
        "      # output_clean2 = model2(Variable(images))\n",
        "      # output_clean3 = model3(Variable(images))\n",
        "    images_adv = Variable(images.data,requires_grad = True)\n",
        "    for j in range(iterations):  \n",
        "      if torch.cuda.is_available():\n",
        "        images_adv = images_adv.cuda()\n",
        "      outputs = model(images_adv)\n",
        "      # outputs2 = model2(images_adv)\n",
        "      # outputs3 = model3(images_adv)\n",
        "      loss = criterion(outputs,Variable(labels))\n",
        "      # loss2 = criterion(outputs2,Variable(labels))\n",
        "      # loss3 = criterion(outputs3,Variable(labels))\n",
        "      model.zero_grad()\n",
        "      # model2.zero_grad()\n",
        "      # model3.zero_grad()\n",
        "      if images_adv.grad is not None:\n",
        "        images.adv.grad.data.fill_(0)\n",
        "      loss.backward()\n",
        "      grad = torch.sign(images_adv.grad.data)  \n",
        "      images_adv = images_adv + (epsilon/iterations)*grad  \n",
        "\n",
        "      images_adv = torch.where(images_adv > images + epsilon,images+epsilon,images_adv)\n",
        "      images_adv = torch.where(images_adv < images-epsilon,images-epsilon,images_adv)\n",
        "      images_adv = torch.clamp(images_adv,min_val,max_val)\n",
        "      images_adv = Variable(images_adv.data,requires_grad = True)\n",
        "      \n",
        "    adv_output = model(Variable(images_adv))\n",
        "    # adv_output2 = model2(Variable(images_adv))\n",
        "    # adv_output3 = model3(Variable(images_adv))\n",
        "    \n",
        "    _,predicted = torch.max(output_clean.data,1)\n",
        "    # _,predicted2 = torch.max(output_clean2.data,1)   \n",
        "    # _,predicted3 = torch.max(output_clean3.data,1)      \n",
        "    _,adv_predicted = torch.max(adv_output.data,1)\n",
        "    # _,adv_predicted2 = torch.max(adv_output2.data,1) \n",
        "    # _,adv_predicted3 = torch.max(adv_output3.data,1)  \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    # correct2 += (predicted2 == labels).sum().item()\n",
        "    # correct3 += (predicted3 == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    # adv_correct2 += (adv_predicted2 == labels).sum().item()\n",
        "    # adv_correct3 += (adv_predicted3 == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    # misclassified2 += (predicted2 != adv_predicted2).sum().item()\n",
        "    # misclassified3 += (predicted3 != adv_predicted3).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.0625, min_val = -1,max_val = 1) \n",
        "FGSM(test_cifar_loader, epsilon = 0.0625, min_val = -1, max_val = 1)"
      ],
      "metadata": {
        "id": "qWo8_pj4b4y9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "70917427-3951-4e77-e14f-6787e44ca127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b02c05c996c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgooglenet_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'googlenet_mod' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TRANSFER ATTACK ON VGG!!_ADV\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "model = model_adv2\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,epsilon = 0.0625, min_val = -1, max_val = 1):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = model(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if images.grad is not None:\n",
        "      images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,min_val,max_val)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = model(Variable(images_adv)) \n",
        "    _,predicted = torch.max(outputs.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)    \n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  ITERATIVE - FGSM\n",
        "\n",
        "def i_FGSM(test_loader,iterations = 1,epsilon = 0.1,min_val = -1,max_val = 1):\n",
        "  correct = 0              \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      output_clean = model(Variable(images))\n",
        "      # output_clean2 = model2(Variable(images))\n",
        "      # output_clean3 = model3(Variable(images))\n",
        "    images_adv = Variable(images.data,requires_grad = True)\n",
        "    for j in range(iterations):  \n",
        "      if torch.cuda.is_available():\n",
        "        images_adv = images_adv.cuda()\n",
        "      outputs = model(images_adv)\n",
        "      # outputs2 = model2(images_adv)\n",
        "      # outputs3 = model3(images_adv)\n",
        "      loss = criterion(outputs,Variable(labels))\n",
        "      # loss2 = criterion(outputs2,Variable(labels))\n",
        "      # loss3 = criterion(outputs3,Variable(labels))\n",
        "      model.zero_grad()\n",
        "      # model2.zero_grad()\n",
        "      # model3.zero_grad()\n",
        "      if images_adv.grad is not None:\n",
        "        images.adv.grad.data.fill_(0)\n",
        "      loss.backward()\n",
        "      grad = torch.sign(images_adv.grad.data)  \n",
        "      images_adv = images_adv + (epsilon/iterations)*grad  \n",
        "\n",
        "      images_adv = torch.where(images_adv > images + epsilon,images+epsilon,images_adv)\n",
        "      images_adv = torch.where(images_adv < images-epsilon,images-epsilon,images_adv)\n",
        "      images_adv = torch.clamp(images_adv,min_val,max_val)\n",
        "      images_adv = Variable(images_adv.data,requires_grad = True)\n",
        "      \n",
        "    adv_output = model(Variable(images_adv))\n",
        "    # adv_output2 = model2(Variable(images_adv))\n",
        "    # adv_output3 = model3(Variable(images_adv))\n",
        "    \n",
        "    _,predicted = torch.max(output_clean.data,1)\n",
        "    # _,predicted2 = torch.max(output_clean2.data,1)   \n",
        "    # _,predicted3 = torch.max(output_clean3.data,1)      \n",
        "    _,adv_predicted = torch.max(adv_output.data,1)\n",
        "    # _,adv_predicted2 = torch.max(adv_output2.data,1) \n",
        "    # _,adv_predicted3 = torch.max(adv_output3.data,1)  \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    # correct2 += (predicted2 == labels).sum().item()\n",
        "    # correct3 += (predicted3 == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    # adv_correct2 += (adv_predicted2 == labels).sum().item()\n",
        "    # adv_correct3 += (adv_predicted3 == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    # misclassified2 += (predicted2 != adv_predicted2).sum().item()\n",
        "    # misclassified3 += (predicted3 != adv_predicted3).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.015625, min_val = -1,max_val = 1) \n",
        "FGSM(test_cifar_loader, epsilon = 0.015625, min_val = -1, max_val = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6UYw4s4wgqb",
        "outputId": "fd326957-334c-4b38-c8e2-dc5558cc2d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model w/0 adverserial attack on test images is : 94.0 %\n",
            "Accuracy of the model with adverserial attack on test images is : 39.53 %\n",
            "Number of misclassified examples(as compared to clean predictions): 5478/10000\n",
            "Accuracy of the model w/0 adverserial attack on test images is : 94.0 %\n",
            "Accuracy of the model with adverserial attack on test images is : 59.15 %\n",
            "Number of misclassified examples(as compared to clean predictions): 3525/10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vgg import vgg11_bn\n",
        "vgg11 = vgg11_bn(pretrained=True).to(device)\n"
      ],
      "metadata": {
        "id": "QL8lNXuyRWDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TRANSFER ATTACK ON VGG!!_ADV\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "model = vgg11_adv\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,epsilon = 0.0625, min_val = -1, max_val = 1):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = resnet18_mod(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if images.grad is not None:\n",
        "      images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,min_val,max_val)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = model(Variable(images_adv)) \n",
        "    _,predicted = torch.max(outputs.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)    \n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  ITERATIVE - FGSM\n",
        "\n",
        "def i_FGSM(test_loader,iterations = 1,epsilon = 0.1,min_val = -1,max_val = 1):\n",
        "  correct = 0              \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      output_clean = model(Variable(images))\n",
        "      # output_clean2 = model2(Variable(images))\n",
        "      # output_clean3 = model3(Variable(images))\n",
        "    images_adv = Variable(images.data,requires_grad = True)\n",
        "    for j in range(iterations):  \n",
        "      if torch.cuda.is_available():\n",
        "        images_adv = images_adv.cuda()\n",
        "      outputs = resnet18_mod(images_adv)\n",
        "      # outputs2 = model2(images_adv)\n",
        "      # outputs3 = model3(images_adv)\n",
        "      loss = criterion(outputs,Variable(labels))\n",
        "      # loss2 = criterion(outputs2,Variable(labels))\n",
        "      # loss3 = criterion(outputs3,Variable(labels))\n",
        "      model.zero_grad()\n",
        "      # model2.zero_grad()\n",
        "      # model3.zero_grad()\n",
        "      if images_adv.grad is not None:\n",
        "        images.adv.grad.data.fill_(0)\n",
        "      loss.backward()\n",
        "      grad = torch.sign(images_adv.grad.data)  \n",
        "      images_adv = images_adv + (epsilon/iterations)*grad  \n",
        "\n",
        "      images_adv = torch.where(images_adv > images + epsilon,images+epsilon,images_adv)\n",
        "      images_adv = torch.where(images_adv < images-epsilon,images-epsilon,images_adv)\n",
        "      images_adv = torch.clamp(images_adv,min_val,max_val)\n",
        "      images_adv = Variable(images_adv.data,requires_grad = True)\n",
        "      \n",
        "    adv_output = model(Variable(images_adv))\n",
        "    # adv_output2 = model2(Variable(images_adv))\n",
        "    # adv_output3 = model3(Variable(images_adv))\n",
        "    \n",
        "    _,predicted = torch.max(output_clean.data,1)\n",
        "    # _,predicted2 = torch.max(output_clean2.data,1)   \n",
        "    # _,predicted3 = torch.max(output_clean3.data,1)      \n",
        "    _,adv_predicted = torch.max(adv_output.data,1)\n",
        "    # _,adv_predicted2 = torch.max(adv_output2.data,1) \n",
        "    # _,adv_predicted3 = torch.max(adv_output3.data,1)  \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    # correct2 += (predicted2 == labels).sum().item()\n",
        "    # correct3 += (predicted3 == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    # adv_correct2 += (adv_predicted2 == labels).sum().item()\n",
        "    # adv_correct3 += (adv_predicted3 == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    # misclassified2 += (predicted2 != adv_predicted2).sum().item()\n",
        "    # misclassified3 += (predicted3 != adv_predicted3).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "#i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.015625, min_val = -2.11,max_val = 2.11) \n",
        "FGSM(test_cifar_loader, epsilon = 0.015625, min_val = -2.11, max_val = 2.11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQEtvW1aREq9",
        "outputId": "b85d59a8-eb4a-4ae4-ffa4-c156435860b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model w/0 adverserial attack on test images is : 38.72 %\n",
            "Accuracy of the model with adverserial attack on test images is : 68.89 %\n",
            "Number of misclassified examples(as compared to clean predictions): 6623/10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TRANSFER ATTACK ON VGG11 PRE-TRAINED ONLY ON CLEAN DATA\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "model = vgg11\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,epsilon = 0.0625, min_val = -1, max_val = 1):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = mobilenet_v2_mod(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if images.grad is not None:\n",
        "      images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,min_val,max_val)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = model(Variable(images_adv)) \n",
        "    _,predicted = torch.max(outputs.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)    \n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  ITERATIVE - FGSM\n",
        "\n",
        "def i_FGSM(test_loader,iterations = 1,epsilon = 0.1,min_val = -1,max_val = 1):\n",
        "  correct = 0              \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      output_clean = model(Variable(images))\n",
        "      # output_clean2 = model2(Variable(images))\n",
        "      # output_clean3 = model3(Variable(images))\n",
        "    images_adv = Variable(images.data,requires_grad = True)\n",
        "    for j in range(iterations):  \n",
        "      if torch.cuda.is_available():\n",
        "        images_adv = images_adv.cuda()\n",
        "      outputs = model(images_adv)\n",
        "      # outputs2 = model2(images_adv)\n",
        "      # outputs3 = model3(images_adv)\n",
        "      loss = criterion(outputs,Variable(labels))\n",
        "      # loss2 = criterion(outputs2,Variable(labels))\n",
        "      # loss3 = criterion(outputs3,Variable(labels))\n",
        "      model.zero_grad()\n",
        "      # model2.zero_grad()\n",
        "      # model3.zero_grad()\n",
        "      if images_adv.grad is not None:\n",
        "        images.adv.grad.data.fill_(0)\n",
        "      loss.backward()\n",
        "      grad = torch.sign(images_adv.grad.data)  \n",
        "      images_adv = images_adv + (epsilon/iterations)*grad  \n",
        "\n",
        "      images_adv = torch.where(images_adv > images + epsilon,images+epsilon,images_adv)\n",
        "      images_adv = torch.where(images_adv < images-epsilon,images-epsilon,images_adv)\n",
        "      images_adv = torch.clamp(images_adv,min_val,max_val)\n",
        "      images_adv = Variable(images_adv.data,requires_grad = True)\n",
        "      \n",
        "    adv_output = model(Variable(images_adv))\n",
        "    # adv_output2 = model2(Variable(images_adv))\n",
        "    # adv_output3 = model3(Variable(images_adv))\n",
        "    \n",
        "    _,predicted = torch.max(output_clean.data,1)\n",
        "    # _,predicted2 = torch.max(output_clean2.data,1)   \n",
        "    # _,predicted3 = torch.max(output_clean3.data,1)      \n",
        "    _,adv_predicted = torch.max(adv_output.data,1)\n",
        "    # _,adv_predicted2 = torch.max(adv_output2.data,1) \n",
        "    # _,adv_predicted3 = torch.max(adv_output3.data,1)  \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    # correct2 += (predicted2 == labels).sum().item()\n",
        "    # correct3 += (predicted3 == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    # adv_correct2 += (adv_predicted2 == labels).sum().item()\n",
        "    # adv_correct3 += (adv_predicted3 == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    # misclassified2 += (predicted2 != adv_predicted2).sum().item()\n",
        "    # misclassified3 += (predicted3 != adv_predicted3).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "# i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.0625, min_val = -1,max_val = 1) \n",
        "# FGSM(test_cifar_loader, epsilon = 0.0625, min_val = -1, max_val = 1)\n",
        "\n",
        "FGSM(test_cifar_loader, epsilon = 0.015625, min_val = -2.11, max_val = 2.11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hntAT6mES9dT",
        "outputId": "95b469ff-af1c-4b58-ef31-587e91f093d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model w/0 adverserial attack on test images is : 46.37 %\n",
            "Accuracy of the model with adverserial attack on test images is : 85.18 %\n",
            "Number of misclassified examples(as compared to clean predictions): 5520/10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TRANSFER ATTACK ON VGG11 PRE-TRAINED ONLY ON CLEAN DATA\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "model = vgg11\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,epsilon = 0.0625, min_val = -1, max_val = 1):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = resnet18_mod(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if images.grad is not None:\n",
        "      images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,min_val,max_val)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = model(Variable(images_adv)) \n",
        "    _,predicted = torch.max(outputs.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)    \n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  ITERATIVE - FGSM\n",
        "\n",
        "def i_FGSM(test_loader,iterations = 1,epsilon = 0.1,min_val = -1,max_val = 1):\n",
        "  correct = 0              \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      output_clean = model(Variable(images))\n",
        "      # output_clean2 = model2(Variable(images))\n",
        "      # output_clean3 = model3(Variable(images))\n",
        "    images_adv = Variable(images.data,requires_grad = True)\n",
        "    for j in range(iterations):  \n",
        "      if torch.cuda.is_available():\n",
        "        images_adv = images_adv.cuda()\n",
        "      outputs = model(images_adv)\n",
        "      # outputs2 = model2(images_adv)\n",
        "      # outputs3 = model3(images_adv)\n",
        "      loss = criterion(outputs,Variable(labels))\n",
        "      # loss2 = criterion(outputs2,Variable(labels))\n",
        "      # loss3 = criterion(outputs3,Variable(labels))\n",
        "      model.zero_grad()\n",
        "      # model2.zero_grad()\n",
        "      # model3.zero_grad()\n",
        "      if images_adv.grad is not None:\n",
        "        images.adv.grad.data.fill_(0)\n",
        "      loss.backward()\n",
        "      grad = torch.sign(images_adv.grad.data)  \n",
        "      images_adv = images_adv + (epsilon/iterations)*grad  \n",
        "\n",
        "      images_adv = torch.where(images_adv > images + epsilon,images+epsilon,images_adv)\n",
        "      images_adv = torch.where(images_adv < images-epsilon,images-epsilon,images_adv)\n",
        "      images_adv = torch.clamp(images_adv,min_val,max_val)\n",
        "      images_adv = Variable(images_adv.data,requires_grad = True)\n",
        "      \n",
        "    adv_output = model(Variable(images_adv))\n",
        "    # adv_output2 = model2(Variable(images_adv))\n",
        "    # adv_output3 = model3(Variable(images_adv))\n",
        "    \n",
        "    _,predicted = torch.max(output_clean.data,1)\n",
        "    # _,predicted2 = torch.max(output_clean2.data,1)   \n",
        "    # _,predicted3 = torch.max(output_clean3.data,1)      \n",
        "    _,adv_predicted = torch.max(adv_output.data,1)\n",
        "    # _,adv_predicted2 = torch.max(adv_output2.data,1) \n",
        "    # _,adv_predicted3 = torch.max(adv_output3.data,1)  \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    # correct2 += (predicted2 == labels).sum().item()\n",
        "    # correct3 += (predicted3 == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    # adv_correct2 += (adv_predicted2 == labels).sum().item()\n",
        "    # adv_correct3 += (adv_predicted3 == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    # misclassified2 += (predicted2 != adv_predicted2).sum().item()\n",
        "    # misclassified3 += (predicted3 != adv_predicted3).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "# i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.0625, min_val = -1,max_val = 1) \n",
        "# FGSM(test_cifar_loader, epsilon = 0.0625, min_val = -1, max_val = 1)\n",
        "\n",
        "FGSM(test_cifar_loader, epsilon = 0.015625, min_val = -2.11, max_val = 2.11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNxl6bhl1RMB",
        "outputId": "6fbaed88-90fa-4f1e-fe3e-b54274258d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model w/0 adverserial attack on test images is : 38.72 %\n",
            "Accuracy of the model with adverserial attack on test images is : 10.52 %\n",
            "Number of misclassified examples(as compared to clean predictions): 9461/10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Final Project/models')\n",
        "vgg11_adv_ens_4 = vgg11_bn()\n",
        "vgg11_adv_ens_4.load_state_dict(torch.load('./state_dicts/vgg11_adv_ens_4.pt'))\n",
        "vgg11_adv_ens_4.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG_cB3_ix3_z",
        "outputId": "2754b490-071d-478c-f755-d762776a9b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU(inplace=True)\n",
              "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg11_adv_256 = vgg11_bn()\n",
        "vgg11_adv_256.load_state_dict(torch.load('./state_dicts/vgg11_adv_256.pt'))\n",
        "vgg11_adv_256.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7MGVmw_2h2n",
        "outputId": "d07c895f-2350-4422-841d-61bbb69d56b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU(inplace=True)\n",
              "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TRANSFER ATTACK ON VGG!!_ADV\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "model = vgg11_adv_256\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,epsilon = 0.0625, min_val = -1, max_val = 1):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = resnet18_mod2(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if images.grad is not None:\n",
        "      images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,min_val,max_val)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = model(Variable(images_adv)) \n",
        "    outputs2 = model(images)\n",
        "    _,predicted = torch.max(outputs2.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)    \n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  ITERATIVE - FGSM\n",
        "\n",
        "def i_FGSM(test_loader,iterations = 1,epsilon = 0.1,min_val = -1,max_val = 1):\n",
        "  correct = 0              \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      output_clean = model(Variable(images))\n",
        "      # output_clean2 = model2(Variable(images))\n",
        "      # output_clean3 = model3(Variable(images))\n",
        "    images_adv = Variable(images.data,requires_grad = True)\n",
        "    for j in range(iterations):  \n",
        "      if torch.cuda.is_available():\n",
        "        images_adv = images_adv.cuda()\n",
        "      outputs = model(images_adv)\n",
        "      # outputs2 = model2(images_adv)\n",
        "      # outputs3 = model3(images_adv)\n",
        "      loss = criterion(outputs,Variable(labels))\n",
        "      # loss2 = criterion(outputs2,Variable(labels))\n",
        "      # loss3 = criterion(outputs3,Variable(labels))\n",
        "      model.zero_grad()\n",
        "      # model2.zero_grad()\n",
        "      # model3.zero_grad()\n",
        "      if images_adv.grad is not None:\n",
        "        images.adv.grad.data.fill_(0)\n",
        "      loss.backward()\n",
        "      grad = torch.sign(images_adv.grad.data)  \n",
        "      images_adv = images_adv + (epsilon/iterations)*grad  \n",
        "\n",
        "      images_adv = torch.where(images_adv > images + epsilon,images+epsilon,images_adv)\n",
        "      images_adv = torch.where(images_adv < images-epsilon,images-epsilon,images_adv)\n",
        "      images_adv = torch.clamp(images_adv,min_val,max_val)\n",
        "      images_adv = Variable(images_adv.data,requires_grad = True)\n",
        "      \n",
        "    adv_output = model(Variable(images_adv))\n",
        "    # adv_output2 = model2(Variable(images_adv))\n",
        "    # adv_output3 = model3(Variable(images_adv))\n",
        "    \n",
        "    _,predicted = torch.max(output_clean.data,1)\n",
        "    # _,predicted2 = torch.max(output_clean2.data,1)   \n",
        "    # _,predicted3 = torch.max(output_clean3.data,1)      \n",
        "    _,adv_predicted = torch.max(adv_output.data,1)\n",
        "    # _,adv_predicted2 = torch.max(adv_output2.data,1) \n",
        "    # _,adv_predicted3 = torch.max(adv_output3.data,1)  \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    # correct2 += (predicted2 == labels).sum().item()\n",
        "    # correct3 += (predicted3 == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    # adv_correct2 += (adv_predicted2 == labels).sum().item()\n",
        "    # adv_correct3 += (adv_predicted3 == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    # misclassified2 += (predicted2 != adv_predicted2).sum().item()\n",
        "    # misclassified3 += (predicted3 != adv_predicted3).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "#i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.0625, min_val = -1,max_val = 1) \n",
        "FGSM(test_cifar_loader, epsilon = 0.015625, min_val = -2.11, max_val = 2.11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8biF-cc2l1D",
        "outputId": "5ea236e5-d354-4077-cc8c-f0eeb41bdd97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model w/0 adverserial attack on test images is : 68.42 %\n",
            "Accuracy of the model with adverserial attack on test images is : 68.14 %\n",
            "Number of misclassified examples(as compared to clean predictions): 402/10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TRANSFER ATTACK ON VGG!! PRETRAINED ON CLEAN DATA \n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "model = vgg11\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,epsilon = 0.0625, min_val = -1, max_val = 1):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = resnet18_mod2(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if images.grad is not None:\n",
        "      images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,min_val,max_val)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = model(Variable(images_adv)) \n",
        "    outputs2 = model(images)\n",
        "    _,predicted = torch.max(outputs2.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)    \n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  ITERATIVE - FGSM\n",
        "\n",
        "def i_FGSM(test_loader,iterations = 1,epsilon = 0.1,min_val = -1,max_val = 1):\n",
        "  correct = 0              \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      output_clean = model(Variable(images))\n",
        "      # output_clean2 = model2(Variable(images))\n",
        "      # output_clean3 = model3(Variable(images))\n",
        "    images_adv = Variable(images.data,requires_grad = True)\n",
        "    for j in range(iterations):  \n",
        "      if torch.cuda.is_available():\n",
        "        images_adv = images_adv.cuda()\n",
        "      outputs = model(images_adv)\n",
        "      # outputs2 = model2(images_adv)\n",
        "      # outputs3 = model3(images_adv)\n",
        "      loss = criterion(outputs,Variable(labels))\n",
        "      # loss2 = criterion(outputs2,Variable(labels))\n",
        "      # loss3 = criterion(outputs3,Variable(labels))\n",
        "      model.zero_grad()\n",
        "      # model2.zero_grad()\n",
        "      # model3.zero_grad()\n",
        "      if images_adv.grad is not None:\n",
        "        images.adv.grad.data.fill_(0)\n",
        "      loss.backward()\n",
        "      grad = torch.sign(images_adv.grad.data)  \n",
        "      images_adv = images_adv + (epsilon/iterations)*grad  \n",
        "\n",
        "      images_adv = torch.where(images_adv > images + epsilon,images+epsilon,images_adv)\n",
        "      images_adv = torch.where(images_adv < images-epsilon,images-epsilon,images_adv)\n",
        "      images_adv = torch.clamp(images_adv,min_val,max_val)\n",
        "      images_adv = Variable(images_adv.data,requires_grad = True)\n",
        "      \n",
        "    adv_output = model(Variable(images_adv))\n",
        "    # adv_output2 = model2(Variable(images_adv))\n",
        "    # adv_output3 = model3(Variable(images_adv))\n",
        "    \n",
        "    _,predicted = torch.max(output_clean.data,1)\n",
        "    # _,predicted2 = torch.max(output_clean2.data,1)   \n",
        "    # _,predicted3 = torch.max(output_clean3.data,1)      \n",
        "    _,adv_predicted = torch.max(adv_output.data,1)\n",
        "    # _,adv_predicted2 = torch.max(adv_output2.data,1) \n",
        "    # _,adv_predicted3 = torch.max(adv_output3.data,1)  \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    # correct2 += (predicted2 == labels).sum().item()\n",
        "    # correct3 += (predicted3 == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    # adv_correct2 += (adv_predicted2 == labels).sum().item()\n",
        "    # adv_correct3 += (adv_predicted3 == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    # misclassified2 += (predicted2 != adv_predicted2).sum().item()\n",
        "    # misclassified3 += (predicted3 != adv_predicted3).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "#i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.0625, min_val = -1,max_val = 1) \n",
        "FGSM(test_cifar_loader, epsilon = 0.015625, min_val = -2.11, max_val = 2.11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUWpq947Bl2B",
        "outputId": "09ffe41e-f62f-4134-cffc-8ede152c6f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model w/0 adverserial attack on test images is : 90.61 %\n",
            "Accuracy of the model with adverserial attack on test images is : 73.62 %\n",
            "Number of misclassified examples(as compared to clean predictions): 1868/10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TRANSFER ATTACK ON VGG11_adv\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "model = vgg11_adv \n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,epsilon = 0.0625, min_val = -1, max_val = 1):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = resnet18_mod2(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if images.grad is not None:\n",
        "      images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,min_val,max_val)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = model(Variable(images_adv)) \n",
        "    outputs2 = model(images)\n",
        "    _,predicted = torch.max(outputs2.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)    \n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  ITERATIVE - FGSM\n",
        "\n",
        "def i_FGSM(test_loader,iterations = 1,epsilon = 0.1,min_val = -1,max_val = 1):\n",
        "  correct = 0              \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      output_clean = model(Variable(images))\n",
        "      # output_clean2 = model2(Variable(images))\n",
        "      # output_clean3 = model3(Variable(images))\n",
        "    images_adv = Variable(images.data,requires_grad = True)\n",
        "    for j in range(iterations):  \n",
        "      if torch.cuda.is_available():\n",
        "        images_adv = images_adv.cuda()\n",
        "      outputs = model(images_adv)\n",
        "      # outputs2 = model2(images_adv)\n",
        "      # outputs3 = model3(images_adv)\n",
        "      loss = criterion(outputs,Variable(labels))\n",
        "      # loss2 = criterion(outputs2,Variable(labels))\n",
        "      # loss3 = criterion(outputs3,Variable(labels))\n",
        "      model.zero_grad()\n",
        "      # model2.zero_grad()\n",
        "      # model3.zero_grad()\n",
        "      if images_adv.grad is not None:\n",
        "        images.adv.grad.data.fill_(0)\n",
        "      loss.backward()\n",
        "      grad = torch.sign(images_adv.grad.data)  \n",
        "      images_adv = images_adv + (epsilon/iterations)*grad  \n",
        "\n",
        "      images_adv = torch.where(images_adv > images + epsilon,images+epsilon,images_adv)\n",
        "      images_adv = torch.where(images_adv < images-epsilon,images-epsilon,images_adv)\n",
        "      images_adv = torch.clamp(images_adv,min_val,max_val)\n",
        "      images_adv = Variable(images_adv.data,requires_grad = True)\n",
        "      \n",
        "    adv_output = model(Variable(images_adv))\n",
        "    # adv_output2 = model2(Variable(images_adv))\n",
        "    # adv_output3 = model3(Variable(images_adv))\n",
        "    \n",
        "    _,predicted = torch.max(output_clean.data,1)\n",
        "    # _,predicted2 = torch.max(output_clean2.data,1)   \n",
        "    # _,predicted3 = torch.max(output_clean3.data,1)      \n",
        "    _,adv_predicted = torch.max(adv_output.data,1)\n",
        "    # _,adv_predicted2 = torch.max(adv_output2.data,1) \n",
        "    # _,adv_predicted3 = torch.max(adv_output3.data,1)  \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    # correct2 += (predicted2 == labels).sum().item()\n",
        "    # correct3 += (predicted3 == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    # adv_correct2 += (adv_predicted2 == labels).sum().item()\n",
        "    # adv_correct3 += (adv_predicted3 == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    # misclassified2 += (predicted2 != adv_predicted2).sum().item()\n",
        "    # misclassified3 += (predicted3 != adv_predicted3).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "#i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.0625, min_val = -1,max_val = 1) \n",
        "FGSM(test_cifar_loader, epsilon = 0.015625, min_val = -2.11, max_val = 2.11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVAothJCBw5c",
        "outputId": "9c38bdab-3570-4c3d-f8f8-67fb3095e2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model w/0 adverserial attack on test images is : 69.33 %\n",
            "Accuracy of the model with adverserial attack on test images is : 68.88 %\n",
            "Number of misclassified examples(as compared to clean predictions): 120/10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TRANSFER ATTACK ON VGG!!_ADV\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "model = vgg11_adv_ens_4\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,epsilon = 0.0625, min_val = -1, max_val = 1):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = mobilenet_v2_mod(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if images.grad is not None:\n",
        "      images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,min_val,max_val)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = model(Variable(images_adv)) \n",
        "    _,predicted = torch.max(outputs.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)    \n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  ITERATIVE - FGSM\n",
        "\n",
        "def i_FGSM(test_loader,iterations = 1,epsilon = 0.1,min_val = -1,max_val = 1):\n",
        "  correct = 0              \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      output_clean = model(Variable(images))\n",
        "      # output_clean2 = model2(Variable(images))\n",
        "      # output_clean3 = model3(Variable(images))\n",
        "    images_adv = Variable(images.data,requires_grad = True)\n",
        "    for j in range(iterations):  \n",
        "      if torch.cuda.is_available():\n",
        "        images_adv = images_adv.cuda()\n",
        "      outputs = model(images_adv)\n",
        "      # outputs2 = model2(images_adv)\n",
        "      # outputs3 = model3(images_adv)\n",
        "      loss = criterion(outputs,Variable(labels))\n",
        "      # loss2 = criterion(outputs2,Variable(labels))\n",
        "      # loss3 = criterion(outputs3,Variable(labels))\n",
        "      model.zero_grad()\n",
        "      # model2.zero_grad()\n",
        "      # model3.zero_grad()\n",
        "      if images_adv.grad is not None:\n",
        "        images.adv.grad.data.fill_(0)\n",
        "      loss.backward()\n",
        "      grad = torch.sign(images_adv.grad.data)  \n",
        "      images_adv = images_adv + (epsilon/iterations)*grad  \n",
        "\n",
        "      images_adv = torch.where(images_adv > images + epsilon,images+epsilon,images_adv)\n",
        "      images_adv = torch.where(images_adv < images-epsilon,images-epsilon,images_adv)\n",
        "      images_adv = torch.clamp(images_adv,min_val,max_val)\n",
        "      images_adv = Variable(images_adv.data,requires_grad = True)\n",
        "      \n",
        "    adv_output = model(Variable(images_adv))\n",
        "    # adv_output2 = model2(Variable(images_adv))\n",
        "    # adv_output3 = model3(Variable(images_adv))\n",
        "    \n",
        "    _,predicted = torch.max(output_clean.data,1)\n",
        "    # _,predicted2 = torch.max(output_clean2.data,1)   \n",
        "    # _,predicted3 = torch.max(output_clean3.data,1)      \n",
        "    _,adv_predicted = torch.max(adv_output.data,1)\n",
        "    # _,adv_predicted2 = torch.max(adv_output2.data,1) \n",
        "    # _,adv_predicted3 = torch.max(adv_output3.data,1)  \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    # correct2 += (predicted2 == labels).sum().item()\n",
        "    # correct3 += (predicted3 == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    # adv_correct2 += (adv_predicted2 == labels).sum().item()\n",
        "    # adv_correct3 += (adv_predicted3 == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    # misclassified2 += (predicted2 != adv_predicted2).sum().item()\n",
        "    # misclassified3 += (predicted3 != adv_predicted3).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "#i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.0625, min_val = -1,max_val = 1) \n",
        "FGSM(test_cifar_loader, epsilon = 0.015625, min_val = -2.11, max_val = 2.11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJrRUmc2TDDt",
        "outputId": "16391c95-02fc-4fb4-c254-3ad64d18245b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model w/0 adverserial attack on test images is : 46.37 %\n",
            "Accuracy of the model with adverserial attack on test images is : 74.58 %\n",
            "Number of misclassified examples(as compared to clean predictions): 5851/10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFER ATTACK ON VGG!!_ADV\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "#model = vgg11_adv_ens_4\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,source_mod, target_mod,epsilon = 0.0625):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = source_mod(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    target_mod.zero_grad()\n",
        "    #if images.grad is not None:\n",
        "    #  images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,0,1)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = target_mod(Variable(images_adv))      \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    #misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    #adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    #y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    #test_images.extend(images.cpu().data.numpy())\n",
        "    #test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  #np.save('adverserial_images.npy',adverserial_images)    \n",
        "  #np.save('y_preds.npy',y_preds)\n",
        "  #np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  #np.save('test_images.npy',test_images)\n",
        "  #np.save('test_label.npy',test_label)\n",
        "  #print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  #print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  #print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "  return adv_correct/total\n",
        "\n",
        "def adversarial_update(images,labels, model,loss, epsilon, step_size, n_iter):\n",
        "  delta = torch.zeros(images.size()).to(device)\n",
        "  image_mod = (images+delta)\n",
        "  for i in range(n_iter):\n",
        "    #image_mod.requires_grad = True\n",
        "    preds = model(image_mod)\n",
        "    loss_val = loss(preds,labels)\n",
        "    if i == 0:\n",
        "      standard_loss = loss_val.item()\n",
        "    loss_val.backward()\n",
        "    gradient = torch.sign(images.grad.data)\n",
        "    delta = delta + step_size*gradient\n",
        "    delta = delta.clamp(min=-epsilon, max=epsilon)\n",
        "    image_mod = (images+delta).clamp(min=0,max=1)\n",
        "  return image_mod\n",
        "\n",
        "def pgd(test_loader,source_mod, target_mod,epsilon = 0.0625, n_iter=15):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = source_mod(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    target_mod.zero_grad()\n",
        "    #if images.grad is not None:\n",
        "    #  images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv =  adversarial_update(images,labels, source_mod,criterion, epsilon, .01, n_iter)   # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = target_mod(Variable(images_adv))      \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    return(adv_correct/total)\n",
        "\n",
        "\n",
        "#  ITERATIVE - FGSM\n",
        "\n",
        "def i_FGSM(test_loader,iterations = 1,epsilon = 0.1,min_val = -1,max_val = 1):\n",
        "  correct = 0              \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      output_clean = model(Variable(images))\n",
        "      # output_clean2 = model2(Variable(images))\n",
        "      # output_clean3 = model3(Variable(images))\n",
        "    images_adv = Variable(images.data,requires_grad = True)\n",
        "    for j in range(iterations):  \n",
        "      if torch.cuda.is_available():\n",
        "        images_adv = images_adv.cuda()\n",
        "      outputs = model(images_adv)\n",
        "      # outputs2 = model2(images_adv)\n",
        "      # outputs3 = model3(images_adv)\n",
        "      loss = criterion(outputs,Variable(labels))\n",
        "      # loss2 = criterion(outputs2,Variable(labels))\n",
        "      # loss3 = criterion(outputs3,Variable(labels))\n",
        "      model.zero_grad()\n",
        "      # model2.zero_grad()\n",
        "      # model3.zero_grad()\n",
        "      if images_adv.grad is not None:\n",
        "        images.adv.grad.data.fill_(0)\n",
        "      loss.backward()\n",
        "      grad = torch.sign(images_adv.grad.data)  \n",
        "      images_adv = images_adv + (epsilon/iterations)*grad  \n",
        "\n",
        "      images_adv = torch.where(images_adv > images + epsilon,images+epsilon,images_adv)\n",
        "      images_adv = torch.where(images_adv < images-epsilon,images-epsilon,images_adv)\n",
        "      images_adv = torch.clamp(images_adv,min_val,max_val)\n",
        "      images_adv = Variable(images_adv.data,requires_grad = True)\n",
        "      \n",
        "    adv_output = model(Variable(images_adv))\n",
        "    # adv_output2 = model2(Variable(images_adv))\n",
        "    # adv_output3 = model3(Variable(images_adv))\n",
        "    \n",
        "    _,predicted = torch.max(output_clean.data,1)\n",
        "    # _,predicted2 = torch.max(output_clean2.data,1)   \n",
        "    # _,predicted3 = torch.max(output_clean3.data,1)      \n",
        "    _,adv_predicted = torch.max(adv_output.data,1)\n",
        "    # _,adv_predicted2 = torch.max(adv_output2.data,1) \n",
        "    # _,adv_predicted3 = torch.max(adv_output3.data,1)  \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    # correct2 += (predicted2 == labels).sum().item()\n",
        "    # correct3 += (predicted3 == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    # adv_correct2 += (adv_predicted2 == labels).sum().item()\n",
        "    # adv_correct3 += (adv_predicted3 == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    # misclassified2 += (predicted2 != adv_predicted2).sum().item()\n",
        "    # misclassified3 += (predicted3 != adv_predicted3).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "#i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.0625, min_val = -1,max_val = 1) \n",
        "holdout_models = [resnet18_mod2, mobilenet_v2_mod2, googlenet_mod2]\n",
        "target_models = [vgg11_ens_adv_indv]#[vgg11, vgg11_adv_ens_4,vgg11_adv_256]\n",
        "target_names = [\"VGG-11\", \"Min-Max Ensemble Adv\", \"Adversarial Training\"]\n",
        "eps_range = np.array([0/255, 4/255,8/255,12/255,16/255,20/255])\n",
        "i = 0\n",
        "all_data_pgd = []\n",
        "all_data_fgsm = []\n",
        "for target in target_models:\n",
        "  print(target_names[i])\n",
        "  i += 1\n",
        "  target_data_fgsm = []\n",
        "  target_data_pgd = []\n",
        "  for epsilon in eps_range:\n",
        "    acc_vec_fgsm = []\n",
        "    acc_vec_pgd = []\n",
        "    for holdout in holdout_models:\n",
        "      acc_vec_fgsm.append(FGSM(test_cifar_loader,holdout,target, epsilon = epsilon))\n",
        "      if epsilon == 0:\n",
        "        acc_vec_pgd.append(pgd(test_cifar_loader,holdout,target, epsilon = epsilon,n_iter=0))\n",
        "      else: \n",
        "        acc_vec_pgd.append(pgd(test_cifar_loader,holdout,target, epsilon = epsilon))\n",
        "      print(\"a\")\n",
        "    target_data_fgsm.append(min(acc_vec_fgsm))\n",
        "    target_data_pgd.append(min(acc_vec_pgd))\n",
        "  all_data_pgd.append(target_data_pgd)\n",
        "  all_data_fgsm.append(target_data_fgsm)\n",
        "  print(all_data_pgd)\n",
        "  print(all_data_fgsm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2bzcAUFC-3y",
        "outputId": "8253f329-e96d-47a8-d906-4c777c008978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG-11\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "a\n",
            "[[0.69, 0.7, 0.7, 0.71, 0.7, 0.68]]\n",
            "[[0.7198, 0.7164, 0.7096, 0.7072, 0.7022, 0.6942]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig1 = plt.figure()\n",
        "orig_fgsm_bb = [0.9047, 0.7367, 0.6186, 0.545, 0.4768, 0.4197]\n",
        "adv_fgsm_bb = [0.6852, 0.6816, 0.6766, 0.6732, 0.6696, 0.6647]\n",
        "advens_fgsm_bb = [0.7198, 0.7164, 0.7096, 0.7072, 0.7022, 0.6942]\n",
        "new_fgsm_bb = [0.7497, 0.7431, 0.738, 0.7307, 0.7228, 0.7145]\n",
        "plt.xlabel(r\"$\\epsilon$\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"FGSM Black Box Transfer Attack\")\n",
        "plt.plot(eps_range, orig_fgsm_bb, '-y', label='VGG-11')\n",
        "plt.plot(eps_range, adv_fgsm_bb, '-k', label='Adversarial Training')\n",
        "plt.plot(eps_range, advens_fgsm_bb, '-r', label='Ensemble Adversarial')\n",
        "plt.plot(eps_range, new_fgsm_bb, '-b', label='Proposed Model')\n",
        "plt.ylim([0,1])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig1.savefig(\"fgsm_bb.png\",dpi=300) \n",
        "\n",
        "fig2 = plt.figure()\n",
        "orig_pgd_bb = [0.92, 0.75, 0.44, 0.25, 0.19, 0.14]\n",
        "adv_pgd_bb = [0.64, 0.64, 0.63, 0.63, 0.62, 0.63]\n",
        "advens_pgd_bb = [0.69, 0.7, 0.7, 0.71, 0.7, 0.68]\n",
        "new_pgd_bb = [0.76, 0.74, 0.73, 0.74, 0.72, 0.71]\n",
        "plt.xlabel(r\"$\\epsilon$\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"PGD Black Box Transfer Attack\")\n",
        "plt.plot(eps_range, orig_pgd_bb, '-y', label='VGG-11')\n",
        "plt.plot(eps_range, adv_pgd_bb, '-k', label='Adversarial Training')\n",
        "plt.plot(eps_range, advens_pgd_bb, '-r', label='Ensemble Adversarial')\n",
        "plt.plot(eps_range, new_pgd_bb, '-b', label='Proposed Model')\n",
        "plt.ylim([0,1])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig2.savefig(\"pgd_bb.png\",dpi=300)\n",
        "\n",
        "fig3 = plt.figure()\n",
        "orig_pgd_wb = [0.93, 0.04, 0.02, 0.0, 0.0, 0.0]\n",
        "adv_pgd_wb = [0.64, 0.41, 0.27, 0.23, 0.17, 0.16]\n",
        "advens_pgd_wb = [0.71, 0.45, 0.41, 0.32, 0.27, 0.26]\n",
        "new_pgd_wb = [0.74, 0.44, 0.42, 0.36, 0.38, 0.36]\n",
        "plt.xlabel(r\"$\\epsilon$\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"PGD White Box Attack\")\n",
        "plt.plot(eps_range, orig_pgd_wb, '-y', label='VGG-11')\n",
        "plt.plot(eps_range, adv_pgd_wb, '-k', label='Adversarial Training')\n",
        "plt.plot(eps_range, advens_pgd_wb, '-r', label='Ensemble Adversarial')\n",
        "plt.plot(eps_range, new_pgd_wb, '-b', label='Proposed Model')\n",
        "plt.ylim([0,1])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig3.savefig(\"pgd_wb.png\",dpi=300)\n",
        "\n",
        "\n",
        "fig4 = plt.figure()\n",
        "orig_fgsm_wb = [0.9239, 0.4294, 0.35, 0.3154, 0.2824, 0.2441]\n",
        "adv_fgsm_wb = [0.6837, 0.4917, 0.4131, 0.3831, 0.3625, 0.3473]\n",
        "advens_fgsm_wb = [0.7227, 0.5486, 0.4977, 0.4761, 0.4627, 0.4488]\n",
        "new_fgsm_wb = [0.7508, 0.61, 0.5826, 0.5645, 0.5493, 0.5314]\n",
        "plt.xlabel(r\"$\\epsilon$\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"FGSM White Box Attack\")\n",
        "plt.plot(eps_range, orig_fgsm_wb, '-y', label='VGG-11')\n",
        "plt.plot(eps_range, adv_fgsm_wb, '-k', label='Adversarial Training')\n",
        "plt.plot(eps_range, advens_fgsm_wb, '-r', label='Ensemble Adversarial')\n",
        "plt.plot(eps_range, new_fgsm_wb, '-b', label='Proposed Model')\n",
        "plt.ylim([0,1])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig4.savefig(\"fgsm_wb.png\",dpi=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pFgGHMmRW_T9",
        "outputId": "d76613d0-b300-48c3-8fdd-2d429f8c03be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_pgd# TRANSFER ATTACK ON VGG!!_ADV\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "model = vgg11_adv\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,epsilon = 0.0625, min_val = -1, max_val = 1):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = mobilenet_v2_mod(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if images.grad is not None:\n",
        "      images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,min_val,max_val)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = model(Variable(images_adv)) \n",
        "    _,predicted = torch.max(outputs.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)    \n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "#i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.0625, min_val = -1,max_val = 1) \n",
        "FGSM(test_cifar_loader, epsilon = 0.0625, min_val = -2.11, max_val = 2.11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw3VzWMTDOe7",
        "outputId": "8b55bc94-db9d-452d-a181-3fa0528bd8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model w/0 adverserial attack on test images is : 46.37 %\n",
            "Accuracy of the model with adverserial attack on test images is : 67.81 %\n",
            "Number of misclassified examples(as compared to clean predictions): 6192/10000\n",
            "Accuracy of the model w/0 adverserial attack on test images is : 46.37 %\n",
            "Accuracy of the model with adverserial attack on test images is : 67.81 %\n",
            "Number of misclassified examples(as compared to clean predictions): 6192/10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSFER ATTACK ON VGG!!_ADV\n",
        "\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "model = vgg11\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
        "\n",
        "# FAST GRADIENT SIGN METHOD (FGSM)\n",
        "def FGSM(test_loader,epsilon = 0.0625, min_val = -1, max_val = 1):\n",
        "  correct = 0                \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0\n",
        "  adv_noise =0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    images = Variable(images,requires_grad = True)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    outputs = mobilenet_v2_mod(images)\n",
        "    loss =criterion(outputs,labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    if images.grad is not None:\n",
        "      images.grad.data.fill_(0)\n",
        "    loss.backward()\n",
        "    \n",
        "    grad = torch.sign(images.grad.data)\n",
        "    images_adv = torch.clamp(images.data + epsilon*grad,min_val,max_val)     # x_adv = x + epsilon*grad\n",
        "    \n",
        "    adv_output = model(Variable(images_adv)) \n",
        "    _,predicted = torch.max(outputs.data,1)     \n",
        "    _,adv_predicted = torch.max(adv_output.data,1) \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)    \n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#  ITERATIVE - FGSM\n",
        "\n",
        "def i_FGSM(test_loader,iterations = 1,epsilon = 0.1,min_val = -1,max_val = 1):\n",
        "  correct = 0              \n",
        "  adv_correct = 0\n",
        "  misclassified = 0\n",
        "  total = 0 \n",
        "  adverserial_images = []\n",
        "  y_preds = []\n",
        "  y_preds_adv = []\n",
        "  test_images = []\n",
        "  test_label = []\n",
        "  \n",
        "  for i, (images,labels) in enumerate(test_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "      output_clean = model(Variable(images))\n",
        "      # output_clean2 = model2(Variable(images))\n",
        "      # output_clean3 = model3(Variable(images))\n",
        "    images_adv = Variable(images.data,requires_grad = True)\n",
        "    for j in range(iterations):  \n",
        "      if torch.cuda.is_available():\n",
        "        images_adv = images_adv.cuda()\n",
        "      outputs = model(images_adv)\n",
        "      # outputs2 = model2(images_adv)\n",
        "      # outputs3 = model3(images_adv)\n",
        "      loss = criterion(outputs,Variable(labels))\n",
        "      # loss2 = criterion(outputs2,Variable(labels))\n",
        "      # loss3 = criterion(outputs3,Variable(labels))\n",
        "      model.zero_grad()\n",
        "      # model2.zero_grad()\n",
        "      # model3.zero_grad()\n",
        "      if images_adv.grad is not None:\n",
        "        images.adv.grad.data.fill_(0)\n",
        "      loss.backward()\n",
        "      grad = torch.sign(images_adv.grad.data)  \n",
        "      images_adv = images_adv + (epsilon/iterations)*grad  \n",
        "\n",
        "      images_adv = torch.where(images_adv > images + epsilon,images+epsilon,images_adv)\n",
        "      images_adv = torch.where(images_adv < images-epsilon,images-epsilon,images_adv)\n",
        "      images_adv = torch.clamp(images_adv,min_val,max_val)\n",
        "      images_adv = Variable(images_adv.data,requires_grad = True)\n",
        "      \n",
        "    adv_output = model(Variable(images_adv))\n",
        "    # adv_output2 = model2(Variable(images_adv))\n",
        "    # adv_output3 = model3(Variable(images_adv))\n",
        "    \n",
        "    _,predicted = torch.max(output_clean.data,1)\n",
        "    # _,predicted2 = torch.max(output_clean2.data,1)   \n",
        "    # _,predicted3 = torch.max(output_clean3.data,1)      \n",
        "    _,adv_predicted = torch.max(adv_output.data,1)\n",
        "    # _,adv_predicted2 = torch.max(adv_output2.data,1) \n",
        "    # _,adv_predicted3 = torch.max(adv_output3.data,1)  \n",
        "    \n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    # correct2 += (predicted2 == labels).sum().item()\n",
        "    # correct3 += (predicted3 == labels).sum().item()\n",
        "    adv_correct += (adv_predicted == labels).sum().item()\n",
        "    # adv_correct2 += (adv_predicted2 == labels).sum().item()\n",
        "    # adv_correct3 += (adv_predicted3 == labels).sum().item()\n",
        "    misclassified += (predicted != adv_predicted).sum().item()\n",
        "    # misclassified2 += (predicted2 != adv_predicted2).sum().item()\n",
        "    # misclassified3 += (predicted3 != adv_predicted3).sum().item()\n",
        "    \n",
        "    adverserial_images.extend((images_adv).cpu().data.numpy())\n",
        "    y_preds.extend(predicted.cpu().data.numpy())\n",
        "    y_preds_adv.extend(adv_predicted.cpu().data.numpy())\n",
        "    test_images.extend(images.cpu().data.numpy())\n",
        "    test_label.extend(labels.cpu().data.numpy())\n",
        "    \n",
        "  np.save('adverserial_images.npy',adverserial_images)\n",
        "  np.save('y_preds.npy',y_preds)\n",
        "  np.save('y_preds_adv.npy',y_preds_adv)\n",
        "  np.save('test_images.npy',test_images)\n",
        "  np.save('test_label.npy',test_label)\n",
        "  print('Accuracy of the model w/0 adverserial attack on test images is : {} %'.format(100*correct/total))\n",
        "  print('Accuracy of the model with adverserial attack on test images is : {} %'.format(100* adv_correct/total))\n",
        "  print('Number of misclassified examples(as compared to clean predictions): {}/{}'.format(misclassified,total))\n",
        "\n",
        "\n",
        "#i_FGSM(test_cifar_loader, iterations = 15,epsilon = 0.0625, min_val = -1,max_val = 1) \n",
        "FGSM(test_cifar_loader, epsilon = 0.0625, min_val = -2.11, max_val = 2.11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6qqrS3BDRDt",
        "outputId": "b20621e4-7b8c-48e4-efcc-a765969beaac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model w/0 adverserial attack on test images is : 46.37 %\n",
            "Accuracy of the model with adverserial attack on test images is : 63.35 %\n",
            "Number of misclassified examples(as compared to clean predictions): 6023/10000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}